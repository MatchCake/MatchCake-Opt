{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AutoML Pipeline from ax - Tutorial",
   "id": "d8672d88f9e3fbef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We start by importing all the necessary packages.",
   "id": "a8a7b08618a6cb87"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from ax import ChoiceParameterConfig, RangeParameterConfig\n",
    "from matchcake import NonInteractingFermionicDevice\n",
    "from matchcake.operations import (\n",
    "    SingleParticleTransitionMatrixOperation,\n",
    ")\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from matchcake_opt.datamodules.datamodule import DataModule\n",
    "from matchcake_opt.modules.classification_model import ClassificationModel\n",
    "from matchcake_opt.tr_pipeline.automl_pipeline import AutoMLPipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we implement our model. In the next cell, you will see an example of classification model from the lightning tutorial. The only difference now is that we specified the hyperparameter configuration of our model in the static attribute `HP_CONFIGS`. It's really important that you override this attribute for all your model since the `AutoMLPipeline` will use it to learn the best hyperparameters set for your model.",
   "id": "516aa68db02de3ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NIFCNN(ClassificationModel):\n",
    "    \"\"\"\n",
    "    Non-Interacting fermions classifier through CNN hamiltonian encoding. The flow of information\n",
    "    in the model goes as follows.\n",
    "\n",
    "    1. CNN(X_{bchw}) -> X_{be}: Build the embeddings with the CNN encoder\n",
    "    2. Linear(X_{be}) -> W_{bn}: Build the local fields weights from the embeddings\n",
    "    3. Linear(X_{be}) -> W_{bt}: Build the ZZ couplings weights from the embeddings\n",
    "    - We now have the hamiltonian: H(X_{be}) = W_{bn} Z_{n} + W_{bt} ZZ_{t}\n",
    "    4. P_{knu}, P_{ktv}: We compute the probabilities to get the eigenstates of Z and ZZ\n",
    "    5. E_{k} = W_{bn} P_{knu} Lambda_{u} + W_{bt} P_{ktv} Lambda_{v}: We now compute the expected values with Lambda_{u} and Lambda_{v} the eigenvalues of Z and ZZ respectively.\n",
    "    6. Softmax(E_{k}) -> Y_{k}: Finally, we apply a softmax to get the probabilities of each class.\n",
    "    \"\"\"\n",
    "    MODEL_NAME = \"NIFCNN\"\n",
    "    DEFAULT_N_QUBITS = 16\n",
    "    DEFAULT_LEARNING_RATE = 2e-4\n",
    "    DEFAULT_ENCODER_OUTPUT_ACTIVATION = \"Tanh\"\n",
    "    MIN_INPUT_SIZE = (28, 28)\n",
    "\n",
    "    HP_CONFIGS = [\n",
    "        RangeParameterConfig(\n",
    "            name=\"learning_rate\",\n",
    "            parameter_type=\"float\",\n",
    "            bounds=(1e-5, 0.1),\n",
    "        ),\n",
    "        RangeParameterConfig(\n",
    "            name=\"n_qubits\",\n",
    "            parameter_type=\"int\",\n",
    "            bounds=(4, 92),\n",
    "        ),\n",
    "        ChoiceParameterConfig(\n",
    "            name=\"encoder_output_activation\",\n",
    "            parameter_type=\"str\",\n",
    "            values=[\"Tanh\", \"Identity\"],\n",
    "            is_ordered=False,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape: Optional[tuple[int, ...]],\n",
    "            output_shape: Optional[tuple[int, ...]],\n",
    "            learning_rate: float = DEFAULT_LEARNING_RATE,\n",
    "            n_qubits: int = DEFAULT_N_QUBITS,\n",
    "            encoder_output_activation: str = DEFAULT_ENCODER_OUTPUT_ACTIVATION,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(input_shape=input_shape, output_shape=output_shape, learning_rate=learning_rate, **kwargs)\n",
    "        self.save_hyperparameters(\"learning_rate\", \"n_qubits\", \"encoder_output_activation\")\n",
    "        self.n_qubits = n_qubits\n",
    "        self.R_DTYPE = torch.float32\n",
    "        self.C_DTYPE = torch.cfloat\n",
    "        self._n_params = np.triu_indices(2 * self.n_qubits, k=1)[0].size\n",
    "        self.q_device = NonInteractingFermionicDevice(\n",
    "            wires=self.n_qubits, r_dtype=self.R_DTYPE, c_dtype=self.C_DTYPE, show_progress=False\n",
    "        )\n",
    "        self.encoder_output_activation = encoder_output_activation\n",
    "        self.input_resize = Resize(self.MIN_INPUT_SIZE)\n",
    "        self.local_fields_encoder = torch.nn.Sequential(\n",
    "            torch.nn.LazyConv2d(512, kernel_size=7),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "            torch.nn.LazyConv2d(128, kernel_size=5),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "            torch.nn.LazyConv2d(64, kernel_size=3),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "        )\n",
    "\n",
    "        self.local_fields_head = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(self.n_qubits),\n",
    "            getattr(torch.nn, encoder_output_activation)()\n",
    "        )\n",
    "        self._hamiltonian_n_params = np.triu_indices(self.n_qubits, k=1)[0].size\n",
    "        self.zz_body_couplings_head = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(self._hamiltonian_n_params),\n",
    "            getattr(torch.nn, encoder_output_activation)()\n",
    "        )\n",
    "\n",
    "        self.zz_body_coupling_weights = torch.nn.Parameter(torch.randn(self._hamiltonian_n_params), requires_grad=True)\n",
    "\n",
    "        self.local_fields_op_eigvals = torch.nn.Parameter(torch.from_numpy(np.array([1.0, -1.0])).float(),\n",
    "                                                          requires_grad=False)  # eigvals(Z)\n",
    "        self.zz_eigvals = torch.nn.Parameter(torch.from_numpy(np.array([1.0, -1.0, -1.0, 1.0])).float(),\n",
    "                                             requires_grad=False)  # eigvals(ZZ)\n",
    "\n",
    "        self.local_fields_wires = [[i] for i in range(self.n_qubits)]\n",
    "        self.couplings_wires = [[i, j] for i, j in np.vstack(np.triu_indices(self.n_qubits, k=1)).T]\n",
    "\n",
    "        self.weights = torch.nn.Parameter(torch.rand((int(self.output_size), self._n_params)), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.weights)\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        dummy_input = torch.randn((3, *self.input_shape)).to(device=self.device)\n",
    "        with torch.no_grad():\n",
    "            self(dummy_input)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.preprocess_input(x)\n",
    "        embeddings = self.local_fields_encoder(x)\n",
    "\n",
    "        local_fields = self.local_fields_head(embeddings)\n",
    "        zz_couplings = self.zz_body_couplings_head(embeddings)\n",
    "        self.q_device.execute_generator(self.circuit_gen(), reset=True)\n",
    "\n",
    "        local_fields_probs = (\n",
    "            self.q_device.probability(wires=self.local_fields_wires)\n",
    "            .to(dtype=self.q_device.R_DTYPE, device=self.torch_device)\n",
    "        )\n",
    "        couplings_probs = (\n",
    "            self.q_device.probability(wires=self.couplings_wires)\n",
    "            .to(dtype=self.q_device.R_DTYPE, device=self.torch_device)\n",
    "        )\n",
    "        weighted_local_eigvals = torch.einsum(\n",
    "            \"bi,kij,j->bk\", local_fields, local_fields_probs, self.local_fields_op_eigvals\n",
    "        )\n",
    "        weighted_zz_eigvals = torch.einsum(\n",
    "            \"bi,kij,j->bk\", zz_couplings, couplings_probs, self.zz_eigvals\n",
    "        )\n",
    "\n",
    "        expval = weighted_local_eigvals + weighted_zz_eigvals\n",
    "        return expval\n",
    "\n",
    "    def circuit_gen(self):\n",
    "        yield qml.BasisState(self.initial_basis_state, wires=self.wires)\n",
    "        yield self.get_sptm_weights()\n",
    "        return\n",
    "\n",
    "    def get_sptm_weights(self):\n",
    "        \"\"\"\n",
    "        Compute the single-particle transition matrix (SPTM) weights based on the initialized weight tensor.\n",
    "\n",
    "        This method constructs a tensor, `h`, which encodes the pairwise weight interactions\n",
    "        in a prescribed upper triangular form. It then symmetrizes `h` to ensure anti-symmetry\n",
    "        about the main diagonal. Afterward, the matrix exponential of `h` is computed to generate\n",
    "        the SPTM. Finally, the SPTM is encapsulated in a `SingleParticleTransitionMatrixOperation`\n",
    "        object for further usage.\n",
    "\n",
    "        :return: An instance of `SingleParticleTransitionMatrixOperation` that encapsulates\n",
    "            the computed single-particle transition matrix based on the initialized weights.\n",
    "            The shape of the SPTM is (n_classes, 2 * n_qubits, 2 * n_qubits).\n",
    "        :rtype: SingleParticleTransitionMatrixOperation\n",
    "        \"\"\"\n",
    "        h = torch.zeros(\n",
    "            (int(self.output_size), 2 * self.n_qubits, 2 * self.n_qubits), dtype=self.R_DTYPE, device=self.torch_device\n",
    "        )\n",
    "        triu_indices = np.triu_indices(2 * self.n_qubits, k=1)\n",
    "        h[:, triu_indices[0], triu_indices[1]] = self.weights\n",
    "        h = h - h.mT\n",
    "        sptm = torch.matrix_exp(h)\n",
    "        return SingleParticleTransitionMatrixOperation(sptm, wires=self.wires)\n",
    "\n",
    "    def preprocess_input(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-2], x.shape[-1])\n",
    "        if x.shape[-2] < self.MIN_INPUT_SIZE[-2] or x.shape[-1] < self.MIN_INPUT_SIZE[-1]:\n",
    "            x = self.input_resize(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def torch_device(self):\n",
    "        return self.device\n",
    "\n",
    "    @property\n",
    "    def wires(self):\n",
    "        return self.q_device.wires\n",
    "\n",
    "    @property\n",
    "    def initial_basis_state(self):\n",
    "        return np.zeros(self.n_qubits, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return int(np.prod(self.output_shape))\n"
   ],
   "id": "a8d028514631ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we define our hyperparameters. Notice this time, we don't have any hyperparameters for our model. This is normal since our goal is to optimize those through the automl pipeline.",
   "id": "c3d5116fc1e4e7d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "dataset_name = \"Digits2D\"\n",
    "fold_id = 0\n",
    "batch_size = 32\n",
    "random_state = 0\n",
    "num_workers = 0\n",
    "\n",
    "# Model\n",
    "model_cls = NIFCNN\n",
    "\n",
    "# Pipeline\n",
    "job_output_folder = Path(os.getcwd()) / \"data\" / \"automl\" / dataset_name / model_cls.MODEL_NAME\n",
    "checkpoint_folder = Path(job_output_folder) / \"checkpoints\"\n",
    "pipeline_args = dict(\n",
    "    max_epochs=100,  # increase at least to 256\n",
    "    max_time=\"00:00:02:00\",  # DD:HH:MM:SS, increase at least to \"00:01:00:00\"\n",
    ")"
   ],
   "id": "d8db16a0825411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we instantiate our datamodule and our automl pipline. Here some explanation on some of the parameters:\n",
    "\n",
    "- `automl_iterations`: That's the number of hyperparameters configuration the pipeline will try before choosing the best one.\n",
    "- `inner_max_epochs`: That's the number of epochs each configuration will be trained for.\n",
    "- `inner_max_time`: That's the maximum time each configuration will be trained for."
   ],
   "id": "c858f32aaebaadb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datamodule = DataModule.from_dataset_name(\n",
    "    dataset_name,\n",
    "    fold_id=fold_id,\n",
    "    batch_size=batch_size,\n",
    "    random_state=random_state,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "automl_pipeline = AutoMLPipeline(\n",
    "    model_cls=model_cls,\n",
    "    datamodule=datamodule,\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    automl_iterations=5,  # increase at least to 32\n",
    "    inner_max_epochs=10,  # increase at least to 128\n",
    "    inner_max_time=\"00:00:01:00\",  # increase at least to \"00:00:10:00\"\n",
    "    automl_overwrite_fit=True,\n",
    "    **pipeline_args\n",
    ")"
   ],
   "id": "ec68f97593517ad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's time to run the optimization.",
   "id": "af11b75e95e08c0c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "start_time = time.perf_counter()\n",
    "automl_pipeline.run()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds\")"
   ],
   "id": "cf7202cce5b169d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then check the best hyperparameters found.",
   "id": "cb990e91a45ed0b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Best Hyperparameters:\\n{json.dumps(automl_pipeline.get_best_params(), indent=2, default=str)}\")",
   "id": "dd640fc80cf084de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can run a full training on the best configuration found.",
   "id": "5f1faa7bc351b3a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lt_pipeline, metrics = automl_pipeline.run_best_pipeline()\n",
    "print(\"⚡\" * 20, \"\\nValidation Metrics:\\n\", metrics, \"\\n\", \"⚡\" * 20)"
   ],
   "id": "e6b042840d5e9104",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And run the test evaluation.",
   "id": "b388c9bdd3f388fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_metrics = lt_pipeline.run_test()\n",
    "print(\"⚡\" * 20, \"\\nTest Metrics:\\n\", test_metrics, \"\\n\", \"⚡\" * 20)"
   ],
   "id": "f4e5d27fa0612fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----------------------------------------",
   "id": "a55f9d611caf576b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

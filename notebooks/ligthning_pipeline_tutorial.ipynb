{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch Lightning Pipeline - Tutorial",
   "id": "5cfde9c2669829a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We start by importing all the necessary packages.",
   "id": "3e36c156f4cba021"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from matchcake import NonInteractingFermionicDevice\n",
    "from matchcake.operations import (\n",
    "    SingleParticleTransitionMatrixOperation,\n",
    ")\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from matchcake_opt.datamodules.datamodule import DataModule\n",
    "from matchcake_opt.modules.classification_model import ClassificationModel\n",
    "from matchcake_opt.tr_pipeline.lightning_pipeline import LightningPipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, we implement our model. In the next cell, you will see an example of classification model that encode images into Ising hamiltonians through a CNN network and then use the Non-Interacting Fermionic (NIF) Device from MatchCake to implement a ansatz that will classify the hamiltonians using a VQE-like optimization.\n",
    "\n",
    "This model inherit from the `ClassificationModel` that itself inherit from the `BaseModel` which inherit from the `lightning.LightningModule`. So, everything you know or can learn from the lightning module can be apply here to implement your model. The `BaseModel` or `ClassificationModel` are wrapper around the `lightning.LightningModule` used to simplify your life and used to work with the pipelines implemented in this package like the `LightningPipeline` that you will use in this notebook.\n",
    "\n",
    "Normally, you will want to override the static attribute `MODEL_NAME` for the name of your model. Then you will want to implement the constructor (the `__init__` method) where you will put the instantiation of your network/layer and the objects that you will use later for the computation. Finally, you will implement the `forward` method like any other torch module where you will implement the actual computation/forward pass of your model.\n",
    "\n",
    "The other methods that you will want to override are all the public methods from the `lightning.LightningModule` itself. See the lightning documentation for that."
   ],
   "id": "b2ea3056814840a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NIFCNN(ClassificationModel):\n",
    "    \"\"\"\n",
    "    Non-Interacting fermions classifier through CNN hamiltonian encoding. The flow of information\n",
    "    in the model goes as follows.\n",
    "\n",
    "    1. CNN(X_{bchw}) -> X_{be}: Build the embeddings with the CNN encoder\n",
    "    2. Linear(X_{be}) -> W_{bn}: Build the local fields weights from the embeddings\n",
    "    3. Linear(X_{be}) -> W_{bt}: Build the ZZ couplings weights from the embeddings\n",
    "    - We now have the hamiltonian: H(X_{be}) = W_{bn} Z_{n} + W_{bt} ZZ_{t}\n",
    "    4. P_{knu}, P_{ktv}: We compute the probabilities to get the eigenstates of Z and ZZ\n",
    "    5. E_{k} = W_{bn} P_{knu} Lambda_{u} + W_{bt} P_{ktv} Lambda_{v}: We now compute the expected values with Lambda_{u} and Lambda_{v} the eigenvalues of Z and ZZ respectively.\n",
    "    6. Softmax(E_{k}) -> Y_{k}: Finally, we apply a softmax to get the probabilities of each class.\n",
    "    \"\"\"\n",
    "    MODEL_NAME = \"NIFCNN\"\n",
    "    DEFAULT_N_QUBITS = 16\n",
    "    DEFAULT_LEARNING_RATE = 2e-4\n",
    "    DEFAULT_ENCODER_OUTPUT_ACTIVATION = \"Tanh\"\n",
    "    MIN_INPUT_SIZE = (28, 28)\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape: Optional[tuple[int, ...]],\n",
    "            output_shape: Optional[tuple[int, ...]],\n",
    "            learning_rate: float = DEFAULT_LEARNING_RATE,\n",
    "            n_qubits: int = DEFAULT_N_QUBITS,\n",
    "            encoder_output_activation: str = DEFAULT_ENCODER_OUTPUT_ACTIVATION,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(input_shape=input_shape, output_shape=output_shape, learning_rate=learning_rate, **kwargs)\n",
    "        self.save_hyperparameters(\"learning_rate\", \"n_qubits\", \"encoder_output_activation\")\n",
    "        self.n_qubits = n_qubits\n",
    "        self.R_DTYPE = torch.float32\n",
    "        self.C_DTYPE = torch.cfloat\n",
    "        self._n_params = np.triu_indices(2 * self.n_qubits, k=1)[0].size\n",
    "        self.q_device = NonInteractingFermionicDevice(\n",
    "            wires=self.n_qubits, r_dtype=self.R_DTYPE, c_dtype=self.C_DTYPE, show_progress=False\n",
    "        )\n",
    "        self.encoder_output_activation = encoder_output_activation\n",
    "        self.input_resize = Resize(self.MIN_INPUT_SIZE)\n",
    "        self.local_fields_encoder = torch.nn.Sequential(\n",
    "            torch.nn.LazyConv2d(512, kernel_size=7),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "            torch.nn.LazyConv2d(128, kernel_size=5),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "            torch.nn.LazyConv2d(64, kernel_size=3),\n",
    "            torch.nn.LazyBatchNorm2d(),\n",
    "        )\n",
    "\n",
    "        self.local_fields_head = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(self.n_qubits),\n",
    "            getattr(torch.nn, encoder_output_activation)()\n",
    "        )\n",
    "        self._hamiltonian_n_params = np.triu_indices(self.n_qubits, k=1)[0].size\n",
    "        self.zz_body_couplings_head = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(self._hamiltonian_n_params),\n",
    "            getattr(torch.nn, encoder_output_activation)()\n",
    "        )\n",
    "\n",
    "        self.zz_body_coupling_weights = torch.nn.Parameter(torch.randn(self._hamiltonian_n_params), requires_grad=True)\n",
    "\n",
    "        self.local_fields_op_eigvals = torch.nn.Parameter(torch.from_numpy(np.array([1.0, -1.0])).float(),\n",
    "                                                          requires_grad=False)  # eigvals(Z)\n",
    "        self.zz_eigvals = torch.nn.Parameter(torch.from_numpy(np.array([1.0, -1.0, -1.0, 1.0])).float(),\n",
    "                                             requires_grad=False)  # eigvals(ZZ)\n",
    "\n",
    "        self.local_fields_wires = [[i] for i in range(self.n_qubits)]\n",
    "        self.couplings_wires = [[i, j] for i, j in np.vstack(np.triu_indices(self.n_qubits, k=1)).T]\n",
    "\n",
    "        self.weights = torch.nn.Parameter(torch.rand((int(self.output_size), self._n_params)), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.weights)\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        dummy_input = torch.randn((3, *self.input_shape)).to(device=self.device)\n",
    "        with torch.no_grad():\n",
    "            self(dummy_input)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.preprocess_input(x)\n",
    "        embeddings = self.local_fields_encoder(x)\n",
    "\n",
    "        local_fields = self.local_fields_head(embeddings)\n",
    "        zz_couplings = self.zz_body_couplings_head(embeddings)\n",
    "        self.q_device.execute_generator(self.circuit_gen(), reset=True)\n",
    "\n",
    "        local_fields_probs = (\n",
    "            self.q_device.probability(wires=self.local_fields_wires)\n",
    "            .to(dtype=self.q_device.R_DTYPE, device=self.torch_device)\n",
    "        )\n",
    "        couplings_probs = (\n",
    "            self.q_device.probability(wires=self.couplings_wires)\n",
    "            .to(dtype=self.q_device.R_DTYPE, device=self.torch_device)\n",
    "        )\n",
    "        weighted_local_eigvals = torch.einsum(\n",
    "            \"bi,kij,j->bk\", local_fields, local_fields_probs, self.local_fields_op_eigvals\n",
    "        )\n",
    "        weighted_zz_eigvals = torch.einsum(\n",
    "            \"bi,kij,j->bk\", zz_couplings, couplings_probs, self.zz_eigvals\n",
    "        )\n",
    "\n",
    "        expval = weighted_local_eigvals + weighted_zz_eigvals\n",
    "        return expval\n",
    "\n",
    "    def circuit_gen(self):\n",
    "        yield qml.BasisState(self.initial_basis_state, wires=self.wires)\n",
    "        yield self.get_sptm_weights()\n",
    "        return\n",
    "\n",
    "    def get_sptm_weights(self):\n",
    "        \"\"\"\n",
    "        Compute the single-particle transition matrix (SPTM) weights based on the initialized weight tensor.\n",
    "\n",
    "        This method constructs a tensor, `h`, which encodes the pairwise weight interactions\n",
    "        in a prescribed upper triangular form. It then symmetrizes `h` to ensure anti-symmetry\n",
    "        about the main diagonal. Afterward, the matrix exponential of `h` is computed to generate\n",
    "        the SPTM. Finally, the SPTM is encapsulated in a `SingleParticleTransitionMatrixOperation`\n",
    "        object for further usage.\n",
    "\n",
    "        :return: An instance of `SingleParticleTransitionMatrixOperation` that encapsulates\n",
    "            the computed single-particle transition matrix based on the initialized weights.\n",
    "            The shape of the SPTM is (n_classes, 2 * n_qubits, 2 * n_qubits).\n",
    "        :rtype: SingleParticleTransitionMatrixOperation\n",
    "        \"\"\"\n",
    "        h = torch.zeros(\n",
    "            (int(self.output_size), 2 * self.n_qubits, 2 * self.n_qubits), dtype=self.R_DTYPE, device=self.torch_device\n",
    "        )\n",
    "        triu_indices = np.triu_indices(2 * self.n_qubits, k=1)\n",
    "        h[:, triu_indices[0], triu_indices[1]] = self.weights\n",
    "        h = h - h.mT\n",
    "        sptm = torch.matrix_exp(h)\n",
    "        return SingleParticleTransitionMatrixOperation(sptm, wires=self.wires)\n",
    "\n",
    "    def preprocess_input(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-2], x.shape[-1])\n",
    "        if x.shape[-2] < self.MIN_INPUT_SIZE[-2] or x.shape[-1] < self.MIN_INPUT_SIZE[-1]:\n",
    "            x = self.input_resize(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def torch_device(self):\n",
    "        return self.device\n",
    "\n",
    "    @property\n",
    "    def wires(self):\n",
    "        return self.q_device.wires\n",
    "\n",
    "    @property\n",
    "    def initial_basis_state(self):\n",
    "        return np.zeros(self.n_qubits, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return int(np.prod(self.output_shape))\n"
   ],
   "id": "773fa29d353f3b44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, its time to define our training hyperparameters. Since we will use our costume `DataModule` to manipulate the data, we specify the dataset by a string and the other parameters like its presented in the next cell.",
   "id": "def275274f4f00a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "dataset_name = \"Digits2D\"\n",
    "fold_id = 0\n",
    "batch_size = 32\n",
    "random_state = 0\n",
    "num_workers = 0\n",
    "\n",
    "# Model\n",
    "model_cls = NIFCNN\n",
    "model_args = dict(\n",
    "    n_qubits=16,\n",
    "    learning_rate=2e-4,\n",
    "    encoder_output_activation=\"Tanh\",\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "job_output_folder = Path(os.getcwd()) / \"data\" / \"lightning\" / dataset_name / model_cls.MODEL_NAME\n",
    "checkpoint_folder = Path(job_output_folder) / \"checkpoints\""
   ],
   "id": "7650c0c97d2d27a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's now time to instantiate the actual datamodule and the training pipeline. We then pass all our predefined hyperparameters.",
   "id": "3b042e2a3818b17b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datamodule = DataModule.from_dataset_name(\n",
    "    dataset_name,\n",
    "    fold_id=fold_id,\n",
    "    batch_size=batch_size,\n",
    "    random_state=random_state,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "lightning_pipeline = LightningPipeline(\n",
    "    model_cls=model_cls,\n",
    "    datamodule=datamodule,\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    max_epochs=10,\n",
    "    max_time=\"00:00:03:00\",  # DD:HH:MM:SS\n",
    "    overwrite_fit=True,\n",
    "    verbose=True,\n",
    "    **model_args,\n",
    ")"
   ],
   "id": "194d37919aea6ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the next cell, we will run the training pipline. This one will run the training and validation loop as defined in `lightning`, will save the best model by default and will stop at the maximum time specified.",
   "id": "8439dd9c7c060fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_time = time.perf_counter()\n",
    "metrics = lightning_pipeline.run()\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = datetime.timedelta(seconds=end_time - start_time)\n",
    "print(f\"Time taken: {elapsed_time}\")\n",
    "print(\"⚡\" * 20, \"\\nValidation Metrics:\\n\", metrics, \"\\n\", \"⚡\" * 20)"
   ],
   "id": "7d2a933b5d774a43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can test our model on the test set using the best model found during training/validation.",
   "id": "bf48908514112242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_metrics = lightning_pipeline.run_test()\n",
    "print(\"⚡\" * 20, \"\\nTest Metrics:\\n\", test_metrics, \"\\n\", \"⚡\" * 20)"
   ],
   "id": "2939460cd8692a2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----------------------------------------",
   "id": "fcca295bdfc94cfa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
